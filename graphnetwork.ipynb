{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Data Generation** \\\\\n",
        "(as seen in GCNG paper)"
      ],
      "metadata": {
        "id": "6-e69HYDta2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "hlnNSGSZHvoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8tR_h9Qf92-"
      },
      "outputs": [],
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.2.1+cu121.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.2.1+cu121.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "metadata": {
        "id": "ohKWzmq9gTvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmxvU8hsgWAU",
        "outputId": "f528d7ec-70b1-449a-f452-aa4a5ea157d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################  get the whole training dataset\n",
        "\n",
        "#current_path = os.path.abspath('.')\n",
        "cortex_svz_cellcentroids = pd.read_csv(\"cortex_svz_cellcentroids.csv\")\n",
        "############# get batch adjacent matrix\n",
        "cell_view_list = []\n",
        "for view_num in range(7):\n",
        "    cell_view = cortex_svz_cellcentroids[cortex_svz_cellcentroids['Field of View']==view_num]\n",
        "    cell_view_list.append(cell_view)"
      ],
      "metadata": {
        "id": "3bbMJW3zgXLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############ the distribution of distance\n",
        "distance_list_list = []\n",
        "distance_list_list_2 = []\n",
        "print ('calculating distance matrix, it takes a while')\n",
        "for view_num in range(7):\n",
        "    print (view_num)\n",
        "    cell_view = cell_view_list[view_num]\n",
        "    distance_list = []\n",
        "    for j in range(cell_view.shape[0]):\n",
        "        for i in range (cell_view.shape[0]):\n",
        "            if i!=j:\n",
        "                distance_list.append(np.linalg.norm(cell_view.iloc[j][['X','Y']]-cell_view.iloc[i][['X','Y']]))\n",
        "    distance_list_list = distance_list_list + distance_list\n",
        "    distance_list_list_2.append(distance_list)\n",
        "\n",
        "\n",
        "# np.save(current_path+'/seqfish_plus/distance_array.npy',np.array(distance_list_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAu9HHMygZc3",
        "outputId": "f686bb54-0094-4196-e6fc-036ad37fdb2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calculating distance matrix, it takes a while\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spektral"
      ],
      "metadata": {
        "id": "3LcZ3tmz0UmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###try different distance threshold, so that on average, each cell has x neighbor cells, see Tab. S1 for results\n",
        "from scipy import sparse\n",
        "import spektral\n",
        "import pickle\n",
        "import scipy.linalg\n",
        "distance_array = np.array(distance_list_list)\n",
        "for threshold in [140]:#[100,140,180,210,220,260]:#range (210,211):#(100,400,40):\n",
        "    num_big = np.where(distance_array<threshold)[0].shape[0]\n",
        "    print (threshold,num_big,str(num_big/(913*2)))\n",
        "    distance_matrix_threshold_I_list = []\n",
        "    distance_matrix_threshold_W_list = []\n",
        "    from sklearn.metrics.pairwise import euclidean_distances\n",
        "    for view_num in range (7):\n",
        "        cell_view = cell_view_list[view_num]\n",
        "        distance_matrix = euclidean_distances(cell_view[['X','Y']], cell_view[['X','Y']])\n",
        "        distance_matrix_threshold_I = np.zeros(distance_matrix.shape)\n",
        "        distance_matrix_threshold_W = np.zeros(distance_matrix.shape)\n",
        "        for i in range(distance_matrix_threshold_I.shape[0]):\n",
        "            for j in range(distance_matrix_threshold_I.shape[1]):\n",
        "                if distance_matrix[i,j] <= threshold and distance_matrix[i,j] > 0:\n",
        "                    distance_matrix_threshold_I[i,j] = 1\n",
        "                    distance_matrix_threshold_W[i,j] = distance_matrix[i,j]\n",
        "        distance_matrix_threshold_I_list.append(distance_matrix_threshold_I)\n",
        "        distance_matrix_threshold_W_list.append(distance_matrix_threshold_W)\n",
        "    whole_distance_matrix_threshold_I = scipy.linalg.block_diag(distance_matrix_threshold_I_list[0],\n",
        "                                                                distance_matrix_threshold_I_list[1],\n",
        "                                                                distance_matrix_threshold_I_list[2],\n",
        "                                                                distance_matrix_threshold_I_list[3],\n",
        "                                                                distance_matrix_threshold_I_list[4],\n",
        "                                                                distance_matrix_threshold_I_list[5],\n",
        "                                                                distance_matrix_threshold_I_list[6])\n",
        "\n",
        "    '''\n",
        "    ############### get normalized sparse adjacent matrix\n",
        "    distance_matrix_threshold_I_N = spektral.utils.normalized_adjacency(whole_distance_matrix_threshold_I, symmetric=True)\n",
        "    print(type(distance_matrix_threshold_I_N))\n",
        "    distance_matrix_threshold_I_N_crs = sparse.csr_matrix(distance_matrix_threshold_I_N)\n",
        "    with open('whole_FOV_distance_I_N_norm_crs_'+str(threshold), 'wb') as fp:\n",
        "        pickle.dump(distance_matrix_threshold_I_N_crs, fp)'''\n",
        "    ############### get not normalized sparse adjacent matrix\n",
        "    distance_matrix_threshold_I_N = np.float32(whole_distance_matrix_threshold_I) ## do not normalize adjcent matrix\n",
        "    print(type(distance_matrix_threshold_I_N))\n",
        "    distance_matrix_threshold_I_N_crs = sparse.csr_matrix(distance_matrix_threshold_I_N)\n",
        "    with open('whole_FOV_distance_I_N_crs_'+str(threshold), 'wb') as fp:\n",
        "        pickle.dump(distance_matrix_threshold_I_N_crs, fp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFWhlh8igfnT",
        "outputId": "e7093cb8-fe8b-407a-accb-9d3630d160e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "140 4490 2.4589266155531218\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########### read ligand receptor database\n",
        "ligand_list = pd.read_csv('ligand_list2.txt',header  = None)\n",
        "receptor_list = pd.read_csv('receptor_list2.txt',header  = None)\n",
        "LR_pairs = pd.read_csv('ligand_receptor_pairs2.txt',header  = None,sep ='\\t')"
      ],
      "metadata": {
        "id": "MZ64g2IyvRdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cortex_svz_counts = pd.read_csv(\"cortex_svz_counts.csv\")\n",
        "cortex_svz_counts_N =cortex_svz_counts.div(cortex_svz_counts.sum(axis=1)+1, axis='rows')*10**4\n",
        "cortex_svz_counts_N.columns =[i.lower() for i in list(cortex_svz_counts_N)] ## gene expression normalization\n",
        "cortex_svz_cellcentroids = pd.read_csv('cortex_svz_cellcentroids.csv')\n",
        "# cortex_svz_counts_N_FOV = cortex_svz_counts_N"
      ],
      "metadata": {
        "id": "xXYrcpfivZ0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gene_list =[i.lower() for i in list(cortex_svz_counts)]\n",
        "\n",
        "non_LR_list = [i for i in gene_list if i not in list(ligand_list.iloc[:,0]) and i not in list(receptor_list.iloc[:,0])]\n",
        "ovlp_ligand_list = [i for i in gene_list if i in list(ligand_list.iloc[:,0])]\n",
        "ovlp_receptor_list = [i for i in gene_list if i in list(receptor_list.iloc[:,0])]\n",
        "\n",
        "count = 0\n",
        "h_LR = defaultdict(list)\n",
        "for LR_pair_index in range(LR_pairs.shape[0]):\n",
        "    ligand, receptor =  LR_pairs.iloc[LR_pair_index]\n",
        "    if ligand in gene_list and receptor in gene_list:\n",
        "        h_LR[ligand].append(receptor)\n",
        "        count = count + 1"
      ],
      "metadata": {
        "id": "2fH43thwvnlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################### generate training dataset containing both postive and negative samples, where negative samples still in the ligand and receptor set, but not in pair set\n",
        "############################################# to split the LR database completely\n",
        "\n",
        "def generate_LR_pairs (h_LR_original,sub_ligand_list, sub_receptor_list,cortex_svz_counts_N):\n",
        "    h_LR = defaultdict(list)\n",
        "    for ligand in h_LR_original.keys():\n",
        "        if ligand in sub_ligand_list:\n",
        "            for receptor in h_LR_original[ligand]:\n",
        "                if receptor in sub_receptor_list:\n",
        "                    h_LR[ligand].append(receptor)\n",
        "    import random\n",
        "    random.seed(0)\n",
        "    count = 0\n",
        "    gene_pair_list  = []\n",
        "    X_data = []\n",
        "    Y_data = []\n",
        "    sub_ligand_list_ovlp = list(h_LR.keys())\n",
        "    for ligand in sub_ligand_list_ovlp:\n",
        "        for receptor in h_LR[ligand]:\n",
        "            gene_pair_list.append(ligand + '\\t' + receptor)\n",
        "            cell_LR_expression = np.array(cortex_svz_counts_N[[ligand, receptor]]) # postive sample\n",
        "            X_data.append(cell_LR_expression)\n",
        "            Y_data.append(1)\n",
        "            ############## get negative samples\n",
        "            non_pair_receptor_list = [i for i in sub_receptor_list if i not in h_LR[ligand]]\n",
        "            random.seed(count)\n",
        "            random_receptor = random.sample(non_pair_receptor_list, 1)[0]\n",
        "            gene_pair_list.append(ligand + '\\t' + random_receptor)\n",
        "            cell_LR_expression = np.array(cortex_svz_counts_N[[ligand, random_receptor]])\n",
        "            X_data.append(cell_LR_expression)\n",
        "            Y_data.append(0)\n",
        "            count = count + 1\n",
        "    ligand_record = sub_ligand_list_ovlp[0]\n",
        "    gene_pair_index = [0]\n",
        "    count = 0\n",
        "    for gene_pair in gene_pair_list:\n",
        "        ligand = gene_pair.split('\\t')[0]\n",
        "        if ligand == ligand_record:\n",
        "            count = count + 1\n",
        "        else:\n",
        "            gene_pair_index.append(count)\n",
        "            ligand_record = ligand\n",
        "            count = count + 1\n",
        "    gene_pair_index.append(count)\n",
        "    X_data_array = np.array(X_data)\n",
        "    Y_data_array = np.array(Y_data)\n",
        "    gene_pair_list_array = np.array(gene_pair_list)\n",
        "    gene_pair_index_array = np.array(gene_pair_index)\n",
        "    return (X_data_array,Y_data_array,gene_pair_list_array,gene_pair_index_array) ## x data, y data, gene pair name, index to separate pairs by ligand genes"
      ],
      "metadata": {
        "id": "nMlH82VPvoHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########## ten fold cross validation data generation\n",
        "ovlp_ligand_list_cons = ovlp_ligand_list\n",
        "ovlp_receptor_list_cons = ovlp_receptor_list\n",
        "import random\n",
        "random.seed(1)\n",
        "ovlp_ligand_list = random.sample(ovlp_ligand_list_cons,len(ovlp_ligand_list))\n",
        "random.seed(1)\n",
        "ovlp_receptor_list = random.sample(ovlp_receptor_list_cons,len(ovlp_receptor_list))\n",
        "for test_indel in range(1,11): ################## ten fold cross validation\n",
        "    print (test_indel)\n",
        "    ######### completely separate ligand and recpetor genes as mutually  exclusive train and test set\n",
        "    whole_ligand_index = [i for i in range(len(ovlp_ligand_list))]\n",
        "    test_ligand = [i for i in range (int(np.ceil((test_indel-1)*0.1*len(ovlp_ligand_list))),int(np.ceil(test_indel*0.1*len(ovlp_ligand_list))))]\n",
        "    train_ligand= [i for i in whole_ligand_index if i not in test_ligand]\n",
        "    whole_receptor_index = [i for i in range(len(ovlp_receptor_list))]\n",
        "    test_receptor = [i for i in range(int(np.ceil((test_indel - 1) * 0.1 * len(ovlp_receptor_list))),int(np.ceil(test_indel * 0.1 * len(ovlp_receptor_list))))]\n",
        "    train_receptor = [i for i in whole_receptor_index if i not in test_receptor]\n",
        "    X_data_array_train, Y_data_array_train, gene_pair_list_array_train, gene_pair_index_array_train = generate_LR_pairs (h_LR,np.array(ovlp_ligand_list)[train_ligand], np.array(ovlp_receptor_list)[train_receptor],cortex_svz_counts_N)\n",
        "    X_data_array_test, Y_data_array_test, gene_pair_list_array_test, gene_pair_index_array_test = generate_LR_pairs(h_LR, np.array(ovlp_ligand_list)[test_ligand], np.array(ovlp_receptor_list)[test_receptor], cortex_svz_counts_N)\n",
        "\n",
        "    if not os.path.isdir('rand_1_10fold/'):\n",
        "        os.makedirs( 'rand_1_10fold/')\n",
        "    np.save('rand_1_10fold/'+str(test_indel)+'_train_X_data_array.npy', X_data_array_train)\n",
        "    np.save('rand_1_10fold/'+str(test_indel)+'_train_Y_data_array.npy', Y_data_array_train)\n",
        "    np.save('rand_1_10fold/'+str(test_indel)+'_train_gene_pair_list_array.npy', gene_pair_list_array_train)\n",
        "    np.save('rand_1_10fold/'+str(test_indel)+'_train_gene_pair_index_array.npy', gene_pair_index_array_train)\n",
        "    np.save('rand_1_10fold/' + str(test_indel) + '_test_X_data_array.npy',X_data_array_test)\n",
        "    np.save('rand_1_10fold/' + str(test_indel) + '_test_Y_data_array.npy',Y_data_array_test)\n",
        "    np.save('rand_1_10fold/' + str(test_indel) + '_test_gene_pair_list_array.npy',gene_pair_list_array_test)\n",
        "    np.save('rand_1_10fold/' + str(test_indel) + '_test_gene_pair_index_array.npy',gene_pair_index_array_test)"
      ],
      "metadata": {
        "id": "GIRULy2bvtlj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be521caa-0c26-4432-db3d-5a486cb8cbe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRAIN**\n",
        "\n",
        "with PyG (PyTorch Geometric) library\n",
        "# autocrine+ GCNG that uses both exocrine and autocrine gene interactions\n",
        "#For diagonal GCNG, just feed it with a zero matrix instead of adjacent matrix."
      ],
      "metadata": {
        "id": "4-1B23ksCcZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from scipy import interp"
      ],
      "metadata": {
        "id": "JNFe97X6CgQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "import numpy as np\n",
        "from scipy import sparse as sp\n",
        "import pickle\n",
        "threshold = 140\n",
        "with open('whole_FOV_distance_I_N_crs_140', 'rb') as fp:\n",
        "    adj = pickle.load( fp)"
      ],
      "metadata": {
        "id": "EIyCZ1FMC26V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def degree_power(adj, pow):\n",
        "    \"\"\"\n",
        "    Computes \\(D^{p}\\) from the given adjacency matrix. Useful for computing\n",
        "    normalised Laplacians.\n",
        "    :param adj: rank 2 array or sparse matrix\n",
        "    :param pow: exponent to which elevate the degree matrix\n",
        "    :return: the exponentiated degree matrix in sparse DIA format\n",
        "    \"\"\"\n",
        "    degrees = np.power(np.array(adj.sum(1)), pow).flatten()\n",
        "    degrees[np.isinf(degrees)] = 0.\n",
        "    if sp.issparse(adj):\n",
        "        D = sp.diags(degrees)\n",
        "    else:\n",
        "        D = np.diag(degrees)\n",
        "    return D\n",
        "\n",
        "\n",
        "def self_connection_normalized_adjacency(adj, symmetric=True):\n",
        "    \"\"\"\n",
        "    Normalizes the given adjacency matrix using the degree matrix as either\n",
        "    \\(D~^{-1}A~\\) or \\(D~^{-1/2}A~D~^{-1/2}\\) (symmetric normalization).where A~ = A+I\n",
        "    :param adj: rank 2 array or sparse matrix;\n",
        "    :param symmetric: boolean, compute symmetric normalization;\n",
        "    :return: the normalized adjacency matrix.\n",
        "    \"\"\"\n",
        "    if sp.issparse(adj):\n",
        "        I = sp.eye(adj.shape[-1], dtype=adj.dtype)\n",
        "    else:\n",
        "        I = np.eye(adj.shape[-1], dtype=adj.dtype)\n",
        "    A1 = adj + I\n",
        "    if symmetric:\n",
        "        normalized_D = degree_power(A1, -0.5)\n",
        "        output = normalized_D.dot(A1).dot(normalized_D)\n",
        "    else:\n",
        "        normalized_D = degree_power(A1, -1.)\n",
        "        output = normalized_D.dot(A1)\n",
        "    return output\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "w3G1Q6OyC8v0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GCNConv\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "IugHEJLXE2xD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torcheval"
      ],
      "metadata": {
        "id": "CtdlUkwpoit-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torcheval.metrics import BinaryAccuracy"
      ],
      "metadata": {
        "id": "OI3-5B9HoygS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "l2_reg = 5e-7  # Regularization rate for l2\n",
        "learning_rate = 1*1e-6  # Learning rate for SGD\n",
        "batch_size = 32  # Batch size\n",
        "epochs = 5 # Number of training epochs\n",
        "es_patience = 50  # Patience fot early stopping\n",
        "criterion = torch.nn.BCELoss()"
      ],
      "metadata": {
        "id": "t00ULJg0O8_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model definition\n",
        "class GCN(torch.nn.Module):\n",
        "      def __init__(self):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(1234567)\n",
        "        self.conv1 = GCNConv(in_channels=2, out_channels=32, add_self_loops=False, bias=True)\n",
        "        self.conv2 = GCNConv(in_channels= 32, out_channels=32, add_self_loops=False, bias=True)\n",
        "        self.linear1 = torch.nn.Linear(29216,512)\n",
        "        self.linear2 = torch.nn.Linear(512,1)\n",
        "\n",
        "      def forward(self, x, adj):\n",
        "        x = self.conv1(x, adj)\n",
        "        x = x.relu()\n",
        "        #x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, adj)\n",
        "        x = x.relu()\n",
        "        x = x.flatten()\n",
        "        x = self.linear1(x)\n",
        "        x = x.relu()\n",
        "        x = self.linear2(x)\n",
        "        x = x.sigmoid()\n",
        "        return x\n",
        "\n",
        "model = GCN()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGHZYWCAOjS3",
        "outputId": "24c2f043-1a69-4452-e4f0-a13e8775dfa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (conv1): GCNConv(2, 32)\n",
            "  (conv2): GCNConv(32, 32)\n",
            "  (linear1): Linear(in_features=29216, out_features=512, bias=True)\n",
            "  (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for test_indel in range(1,3): ################## ten fold cross validation\n",
        "    X_data_train = np.load('rand_1_10fold/'+str(test_indel)+'_train_X_data_array.npy')\n",
        "    Y_data_train = np.load('rand_1_10fold/'+str(test_indel)+'_train_Y_data_array.npy')\n",
        "    gene_pair_index_train = np.load('rand_1_10fold/'+str(test_indel)+'_train_gene_pair_list_array.npy')\n",
        "    count_setx_train = np.load('rand_1_10fold/'+str(test_indel)+'_train_gene_pair_index_array.npy')\n",
        "    X_data_test = np.load('rand_1_10fold/'+str(test_indel)+'_test_X_data_array.npy')\n",
        "    Y_data_test = np.load('rand_1_10fold/'+str(test_indel)+'_test_Y_data_array.npy')\n",
        "    gene_pair_index_test = np.load('rand_1_10fold/'+str(test_indel)+'_test_gene_pair_list_array.npy')\n",
        "    count_set = np.load('rand_1_10fold/'+str(test_indel)+'_test_gene_pair_index_array.npy')\n",
        "    trainX_index = [i for i in range(Y_data_train.shape[0])]\n",
        "    validation_index = trainX_index[:int(np.ceil(0.2*len(trainX_index)))]\n",
        "    train_index = trainX_index[int(np.ceil(0.2*len(trainX_index))):]\n",
        "    X_train, y_train = X_data_train[train_index],Y_data_train[train_index][:,np.newaxis]\n",
        "    X_val, y_val= X_data_train[validation_index],Y_data_train[validation_index][:,np.newaxis]\n",
        "    X_test, y_test= X_data_test,Y_data_test[:,np.newaxis]\n",
        "\n",
        "    print(type(y_train.flatten()))\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJmobttP3g8o",
        "outputId": "88192b0f-36cd-4946-cd9f-4337eb4e28a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "sgmp8RVU4Ki_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for test_indel in range(1,3): ################## ten fold cross validation\n",
        "    X_data_train = np.load('rand_1_10fold/'+str(test_indel)+'_train_X_data_array.npy')\n",
        "    Y_data_train = np.load('rand_1_10fold/'+str(test_indel)+'_train_Y_data_array.npy')\n",
        "    gene_pair_index_train = np.load('rand_1_10fold/'+str(test_indel)+'_train_gene_pair_list_array.npy')\n",
        "    count_setx_train = np.load('rand_1_10fold/'+str(test_indel)+'_train_gene_pair_index_array.npy')\n",
        "    X_data_test = np.load('rand_1_10fold/'+str(test_indel)+'_test_X_data_array.npy')\n",
        "    Y_data_test = np.load('rand_1_10fold/'+str(test_indel)+'_test_Y_data_array.npy')\n",
        "    gene_pair_index_test = np.load('rand_1_10fold/'+str(test_indel)+'_test_gene_pair_list_array.npy')\n",
        "    count_set = np.load('rand_1_10fold/'+str(test_indel)+'_test_gene_pair_index_array.npy')\n",
        "    trainX_index = [i for i in range(Y_data_train.shape[0])]\n",
        "    validation_index = trainX_index[:int(np.ceil(0.2*len(trainX_index)))]\n",
        "    train_index = trainX_index[int(np.ceil(0.2*len(trainX_index))):]\n",
        "    X_train, y_train = X_data_train[train_index],Y_data_train[train_index][:,np.newaxis]\n",
        "    X_val, y_val= X_data_train[validation_index],Y_data_train[validation_index][:,np.newaxis]\n",
        "    X_test, y_test= X_data_test,Y_data_test[:,np.newaxis]\n",
        "\n",
        "    # X_train, y_train, X_val, y_val, X_test, y_test, adj = mnist.load_data()\n",
        "    # X_train, X_val, X_test = X_train[..., None], X_val[..., None], X_test[..., None]\n",
        "    N = X_train.shape[-2]  # Number of nodes in the graphs\n",
        "    F = X_train.shape[-1]  # Node features dimensionality\n",
        "    n_out = y_train.shape[-1]  # Dimension of the target\n",
        "\n",
        "    fltr = self_connection_normalized_adjacency(adj)\n",
        "\n",
        "    fltr_tensor = sparse_mx_to_torch_sparse_tensor(fltr) # adj to tensor\n",
        "\n",
        "    model = GCN()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
        "\n",
        "    def train(x,y,A):\n",
        "      model.train()\n",
        "      optimizer.zero_grad()\n",
        "      out = model(x, A)\n",
        "      loss = criterion(out, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      return out, loss.item()\n",
        "\n",
        "    current_loss = 0\n",
        "    all_losses = []\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      accuracy_train_list = []\n",
        "      for i in range(len(X_train)):\n",
        "        out, loss = train(torch.tensor(X_train[i], dtype=torch.float32), torch.tensor(y_train[i], dtype=torch.float32), fltr_tensor)\n",
        "        current_loss += loss\n",
        "\n",
        "        # training accuracy, threshold 0.5\n",
        "        output_train_acc = out\n",
        "        output_train_acc[output_train_acc >= 0.5] = 1\n",
        "        output_train_acc[output_train_acc < 0.5] = 0\n",
        "        output_train_acc_int = int(output_train_acc)   # float to int\n",
        "\n",
        "        accuracy_train_list.append(output_train_acc_int)\n",
        "\n",
        "\n",
        "      loss_epoch = current_loss / len(X_train)\n",
        "      all_losses.append(loss_epoch)\n",
        "      current_loss = 0\n",
        "\n",
        "      accuracy_train = accuracy_score(y_train.flatten(), accuracy_train_list)\n",
        "      print(f'Epoch: {epoch:03d}, Loss: {loss_epoch:.4f}, Accuracy: {accuracy_train:.4f}')\n",
        "      #print(f'Epoch: {epoch:03d}, Loss: {loss_epoch:.4f}')\n",
        "\n",
        "\n",
        "    overall_loss = sum(all_losses) / epochs\n",
        "    print(f'OVERALL LOSS: {overall_loss:.4f}')\n",
        "    break\n",
        "\n",
        "\n",
        "    '''\n",
        "    # Model definition\n",
        "    X_in = Input(shape=(N, F))\n",
        "    # Pass A as a fixed tensor, otherwise Keras will complain about inputs of\n",
        "    # different rank.\n",
        "    A_in = Input(tensor=sp_matrix_to_sp_tensor(fltr))\n",
        "\n",
        "    graph_conv = GraphConv(32,activation='elu',kernel_regularizer=l2(l2_reg),use_bias=True)([X_in, A_in])\n",
        "    graph_conv = GraphConv(32,activation='elu',kernel_regularizer=l2(l2_reg),use_bias=True)([graph_conv, A_in])\n",
        "    flatten = Flatten()(graph_conv)\n",
        "    fc = Dense(512, activation='relu')(flatten)\n",
        "    output = Dense(n_out, activation='sigmoid')(fc)\n",
        "\n",
        "    # Build model\n",
        "    model = Model(inputs=[X_in, A_in], outputs=output)\n",
        "    optimizer = Adam(lr=learning_rate)\n",
        "    model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['acc'])\n",
        "    model.summary()\n",
        "\n",
        "    plot_model(model, to_file='gcn_LR_spatial_1.png', show_shapes=True)\n",
        "    save_dir = current_path+'/'+str(test_indel)+'_self_connection_Ycv_LR_as_nega_rg_5-7_lr_1-6_e'+str(epochs)\n",
        "    if not os.path.isdir(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    early_stopping = EarlyStopping(monitor='val_acc', patience=es_patience, verbose=0, mode='auto')\n",
        "    checkpoint1 = ModelCheckpoint(filepath=save_dir + '/weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss',verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "    checkpoint2 = ModelCheckpoint(filepath=save_dir + '/weights.hdf5', monitor='val_acc', verbose=1,save_best_only=True, mode='auto', period=1)\n",
        "    callbacks = [checkpoint2, early_stopping]\n",
        "\n",
        "    # Train model\n",
        "    validation_data = (X_val, y_val)\n",
        "    history = model.fit(X_train,y_train,batch_size=batch_size,validation_data=validation_data,epochs=epochs,callbacks=callbacks)\n",
        "\n",
        "    # Load best model\n",
        "    # Save model and weights\n",
        "\n",
        "    model_name = 'gcn_LR_1.h5'\n",
        "    model_path = os.path.join(save_dir, model_name)\n",
        "    model.save(model_path)\n",
        "    print('Saved trained model at %s ' % model_path)\n",
        "    # Score trained model.\n",
        "    scores = model.evaluate(X_test, y_test, verbose=1,batch_size=batch_size)\n",
        "    print('Test loss:', scores[0])\n",
        "    print('Test accuracy:', scores[1])\n",
        "    y_predict = model.predict(X_test)\n",
        "    np.save(save_dir + '/end_y_test.npy', y_test)\n",
        "    np.save(save_dir + '/end_y_predict.npy', y_predict)\n",
        "    ############################################################################## plot training process\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.grid()\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    plt.grid()\n",
        "    plt.savefig(save_dir + '/end_result.pdf')\n",
        "    ###############################################################\n",
        "    #######################################\n",
        "\n",
        "    #############################################################\n",
        "    #########################\n",
        "    y_testy = y_test\n",
        "    y_predicty = y_predict\n",
        "    fig = plt.figure(figsize=(5, 5))\n",
        "    plt.plot([0, 1], [0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.xlim([0, 1])\n",
        "    plt.xlabel('FP')\n",
        "    plt.ylabel('TP')\n",
        "    # plt.grid()\n",
        "    AUC_set = []\n",
        "    s = open(save_dir + '/divided_interaction.txt', 'w')\n",
        "    tprs = []\n",
        "    mean_fpr = np.linspace(0, 1, 100)  # 3068\n",
        "    for jj in range(len(count_set) - 1):  # len(count_set)-1):\n",
        "        if count_set[jj] < count_set[jj + 1]:\n",
        "            print(test_indel, jj, count_set[jj], count_set[jj + 1])\n",
        "            y_test = y_testy[count_set[jj]:count_set[jj + 1]]\n",
        "            y_predict = y_predicty[count_set[jj]:count_set[jj + 1]]\n",
        "            # Score trained model.\n",
        "            fpr, tpr, thresholds = metrics.roc_curve(y_test, y_predict, pos_label=1)\n",
        "            tprs.append(interp(mean_fpr, fpr, tpr))\n",
        "            tprs[-1][0] = 0.0\n",
        "            # Print ROC curve\n",
        "            plt.plot(fpr, tpr, color='0.5', lw=0.001, alpha=.2)\n",
        "            auc = np.trapz(tpr, fpr)\n",
        "            s.write(str(jj) + '\\t' + str(count_set[jj]) + '\\t' + str(count_set[jj + 1]) + '\\t' + str(auc) + '\\n')\n",
        "            print('AUC:', auc)\n",
        "            AUC_set.append(auc)\n",
        "\n",
        "    mean_tpr = np.median(tprs, axis=0)\n",
        "    mean_tpr[-1] = 1.0\n",
        "    per_tpr = np.percentile(tprs, [25, 50, 75], axis=0)\n",
        "    mean_auc = np.trapz(mean_tpr, mean_fpr)\n",
        "    plt.plot(mean_fpr, mean_tpr, 'k', lw=3, label='median ROC')\n",
        "    plt.title(str(mean_auc))\n",
        "    plt.fill_between(mean_fpr, per_tpr[0, :], per_tpr[2, :], color='g', alpha=.2, label='Quartile')\n",
        "    plt.plot(mean_fpr, per_tpr[0, :], 'g', lw=3, alpha=.2)\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.savefig(save_dir + '/divided_interaction_percentile.pdf')\n",
        "    del fig\n",
        "    fig = plt.figure(figsize=(5, 5))\n",
        "    plt.hist(AUC_set, bins=50)\n",
        "    plt.savefig(save_dir + '/divided_interaction_hist.pdf')\n",
        "    del fig\n",
        "    s.close()'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RZ0NOMHDEkM",
        "outputId": "06bc7da4-3f18-4240-8e98-325df66c81e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/utils/_spmm.py:66: UserWarning: Converting sparse tensor to CSR format for more efficient processing. Consider converting your sparse tensor to CSR format beforehand to avoid repeated conversion (got 'torch.sparse_coo')\n",
            "  warnings.warn(f\"Converting sparse tensor to CSR format for more \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 000, Loss: 0.6815, Accuracy: 0.5716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/utils/_spmm.py:66: UserWarning: Converting sparse tensor to CSR format for more efficient processing. Consider converting your sparse tensor to CSR format beforehand to avoid repeated conversion (got 'torch.sparse_coo')\n",
            "  warnings.warn(f\"Converting sparse tensor to CSR format for more \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 0.6598, Accuracy: 0.6570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/utils/_spmm.py:66: UserWarning: Converting sparse tensor to CSR format for more efficient processing. Consider converting your sparse tensor to CSR format beforehand to avoid repeated conversion (got 'torch.sparse_coo')\n",
            "  warnings.warn(f\"Converting sparse tensor to CSR format for more \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 002, Loss: 0.6453, Accuracy: 0.6718\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/utils/_spmm.py:66: UserWarning: Converting sparse tensor to CSR format for more efficient processing. Consider converting your sparse tensor to CSR format beforehand to avoid repeated conversion (got 'torch.sparse_coo')\n",
            "  warnings.warn(f\"Converting sparse tensor to CSR format for more \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 003, Loss: 0.6331, Accuracy: 0.6803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/utils/_spmm.py:66: UserWarning: Converting sparse tensor to CSR format for more efficient processing. Consider converting your sparse tensor to CSR format beforehand to avoid repeated conversion (got 'torch.sparse_coo')\n",
            "  warnings.warn(f\"Converting sparse tensor to CSR format for more \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 004, Loss: 0.6221, Accuracy: 0.6923\n",
            "OVERALL LOSS: 0.6484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrix"
      ],
      "metadata": {
        "id": "ANQa5M0WPHwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "BE5Zvr4MM6bZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_train.flatten(), accuracy_train_list)\n",
        "tn, fp, fn, tp = cm.ravel()"
      ],
      "metadata": {
        "id": "7mTMjs1ENB4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cm)\n",
        "print(\"True Positive :\",tp)\n",
        "print(\"True Negative :\" ,tn)\n",
        "print(\"False Positive :\" ,fp)\n",
        "print(\"False Negative :\" ,fn)"
      ],
      "metadata": {
        "id": "bAq30oe6PEV-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
